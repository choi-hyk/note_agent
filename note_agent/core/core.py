from operator import itemgetter
import os
import re
import json
from typing import Dict, List, Optional
from datetime import datetime

from note_agent.model import HeadInfo, ProfileLengthInfo, NoteAgentOutput, ExpandedOutput
from note_agent.config import (
    PERSIST_DIR,
    RESULTS_DIR,
    EMBEDDING_MODEL,
    LLM_MODEL,
    CHUNK_OVERLAP,
    CHUNK_SIZE,
    RETRIEVE_K,
    TEMP_SUMMARY,
    TEMPL_COMPLETE,
)

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough


def _ensure_dir(path: str) -> str:
    os.makedirs(path, exist_ok=True)
    return path


_ensure_dir(PERSIST_DIR)
_ensure_dir(RESULTS_DIR)

#------------------------------
# Utils
#------------------------------
def estimate_target_length(example_texts: List[str]) -> ProfileLengthInfo:
    """ì˜ˆì‹œ ê¸€ì„ ê¸°ë°˜ìœ¼ë¡œ ê¸¸ì´ë¥¼ ì¶”ì •í•˜ëŠ” í•¨ìˆ˜

    Args:
        example_text (List[str]): ì˜ˆì‹œ ê¸€ ë¦¬ìŠ¤íŠ¸

    Returns:
        length_info (ProfileLengthInfo): í‰ê· /ìµœì†Œ/ìµœëŒ€ ê¸¸ì´ ì •ë³´
    """
    lengths = [len(t) for t in example_texts if t.strip()]
    if not lengths:
        return ProfileLengthInfo(avg_chars=1200, min_chars=1000, max_chars=1600)
    avg = int(sum(lengths) / len(lengths))
    min_chars = max(1000, int(avg * 0.95))
    max_chars = int(avg * 1.30)
    return ProfileLengthInfo(avg_chars=avg, min_chars=min_chars, max_chars=max_chars)


HEADER_RE = re.compile(r"^(#{1,4})\s+(.+)$", re.MULTILINE)


def define_head_info(example_texts: List[str]) -> List[HeadInfo]:
    """ì˜ˆì‹œ ê¸€ì„ ê¸°ë°˜ìœ¼ë¡œ í—¤ë” ì •ë³´ë¥¼ ì •ì˜í•˜ëŠ” í•¨ìˆ˜

    Args:
        example_texts (List[str]): ì˜ˆì‹œ ê¸€ ë¦¬ìŠ¤íŠ¸

    Returns:
        results (List[ProfileHeadInfo]): í—¤ë” ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    results = []
    seen = set()
    for text in example_texts:
        for m in HEADER_RE.finditer(text):
            level = len(m.group(1))
            title = m.group(2).strip()
            key = (level, title)
            if key in seen:
                continue
            seen.add(key)
            results.append(HeadInfo(level=f"H{level}", title=title))
    return results


def build_or_load_vectorstore(
    example_texts: List[str], persist_dir: str = PERSIST_DIR
) -> Chroma:
    """RAGìš© ë²¡í„°ìŠ¤í† ì–´ë¥¼ êµ¬ì¶•í•˜ì—¬ Chromaë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜

    Args:
        example_texts (List[str]): ì˜ˆì‹œ ê¸€ ë¦¬ìŠ¤íŠ¸
        persist_dir (str, optional): ë²¡í„°ìŠ¤í† ì–´ ì˜êµ¬ì €ìž¥ ê²½ë¡œ. Defaults to PERSIST_DIR.

    Returns:
        vs (Chroma): êµ¬ì¶•ëœ Chroma ë²¡í„°ìŠ¤í† ì–´
    """
    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)

    if (
        os.path.exists(persist_dir)
        and os.path.isdir(persist_dir)
        and os.listdir(persist_dir)
    ):
        return Chroma(embedding_function=embeddings, persist_directory=persist_dir)

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP
    )

    chunks, metas = [], []

    for i, text in enumerate(example_texts, start=1):
        for ch in splitter.split_text(text):
            chunks.append(ch)
            metas.append({"source": f"example_{i}"})

    vs = Chroma.from_texts(
        texts=chunks,
        embedding=embeddings,
        metadatas=metas,
        persist_directory=persist_dir,
    )
    return vs


def summarize_style_rules(example_texts: List[str]) -> str:
    """ì˜ˆì‹œ ê¸€ë“¤ì„ ë¶„ì„í•˜ì—¬ ê³µí†µ ìŠ¤íƒ€ì¼ ê·œì¹™ì„ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜

    Args:
        example_texts (List[str]): ì˜ˆì‹œ ê¸€ ë¦¬ìŠ¤íŠ¸

    Returns:
        style_rules (str): ìš”ì•½ëœ ìŠ¤íƒ€ì¼ ê·œì¹™
    """
    summarizer = ChatOpenAI(model=LLM_MODEL, temperature=TEMP_SUMMARY)
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ë„ˆëŠ” í•œêµ­ì–´ ê¸€ ìŠ¤íƒ€ì¼ ë¶„ì„ê°€ë‹¤. ì•„ëž˜ ì˜ˆì‹œ ê¸€ë“¤ì˜ **ê³µí†µ ìŠ¤íƒ€ì¼ ê·œì¹™**ë§Œ í•œêµ­ì–´ë¡œ 5~8ì¤„ ë¶ˆë¦¿ìœ¼ë¡œ ìš”ì•½í•˜ë¼.\n"
                "- ì–´ì¡°: (~ë‹¤ì²´/ ~ìŠµë‹ˆë‹¤ì²´/ ~ìš”ì²´ ì¤‘ ë¬´ì—‡ì¸ì§€)\n"
                "- ë¬¸ìž¥ ê¸¸ì´Â·í˜¸í¡(ì§§ê²Œ/ë³´í†µ/ê¸¸ê²Œ)\n"
                "- ì ‘ì†ì–´(ì˜ˆ: ê·¸ëŸ¬ë‚˜/ë˜í•œ/ì¦‰ ë“±) ì‚¬ìš© ê²½í–¥\n"
                "- ë‹¨ë½ êµ¬ì„±(ì„œë¡ -ë³¸ë¡ -ê²°ë¡  ì—¬ë¶€, í—¤ë”ì— ë”°ë¥¸ ëª©ì°¨ êµ¬ì„± ì—¬ë¶€, ì˜ˆì‹œ/ì¸ìš© ì‚¬ìš© ì—¬ë¶€)\n"
                "- ì–´íœ˜ í†¤(ë‹´ë°±/ì¹œì ˆ/ì „ë¬¸ì  ë“±)\n"
                "â€» í•œêµ­ì–´ë§Œ ìž‘ì„±í•˜ê³ , ë¶„ì„ ì´ì™¸ì˜ ë¬¸ìž¥ì€ ê¸ˆì§€í•œë‹¤.",
            ),
            ("human", "{examples}"),
        ]
    )
    examples_joined = "\n\n---\n\n".join(example_texts)
    chain = prompt | summarizer
    res = chain.invoke({"examples": examples_joined})
    return res.content.strip()


def save_result(
    result: NoteAgentOutput, results_dir: str = RESULTS_DIR
) -> Dict[str, str]:
    """ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ê³¼ JSONìœ¼ë¡œ ì €ìž¥í•˜ëŠ” í•¨ìˆ˜

    Args:
        result (CompletionOutput): ì™„ì„±ëœ ê¸€ ê²°ê³¼
        results_dir (str, optional): ê²°ê³¼ ì €ìž¥ í´ë” ê²½ë¡œ. Defaults to RESULTS_DIR.

    Returns:
        paths (Dict[str, str]): ì €ìž¥ëœ íŒŒì¼ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬
    """
    os.makedirs(results_dir, exist_ok=True)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")

    md_path = os.path.join(results_dir, f"completed_{ts}.md")
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(result.completed_text)

    json_path = os.path.join(results_dir, f"change_log_{ts}.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(result.change_log.model_dump(), f, ensure_ascii=False, indent=2)

    print(f"\nê²°ê³¼ ì €ìž¥ ì™„ë£Œ:\n- {md_path}\n- {json_path}")
    return {"completed_md": md_path, "change_log_json": json_path}


#------------------------------
# ë©”ì¸ í”„ë¡¬í”„íŠ¸
#------------------------------
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ë„ˆëŠ” ì‚¬ìš©ìžì˜ ê¸€ì„ **ì˜ˆì‹œ ìŠ¤íƒ€ì¼**ë¡œ ì™„ì„±í•˜ëŠ” í•œêµ­ì–´ ë„ìš°ë¯¸ë‹¤.\n"
            "ì•„ëž˜ **ìŠ¤íƒ€ì¼ ê·œì¹™**ì„ ë°˜ë“œì‹œ ì ìš©í•˜ë¼:\n{style_rules}\n\n"
            "ë°˜ë“œì‹œ ì¤€ìˆ˜í•  ê·œì¹™:\n"
            "1) ì‚¬ìš©ìž ì´ˆì•ˆ(user_draft)ì€ **ì˜ë¯¸Â·ì‚¬ì‹¤Â·ë…¼ë¦¬ êµ¬ì¡°ë¥¼ ë³´ì¡´**í•˜ê³ , **ë§žì¶¤ë²•/ë„ì–´ì“°ê¸°/ë¬¸ìž¥ë¶€í˜¸ë§Œ êµì •**í•˜ì—¬ `draft_corrected` í•„ë“œë¡œ ë°˜í™˜í•˜ë¼.\n"
            "3) ì´ì–´ì§ˆ ì¶”ê°€ ë³¸ë¬¸ì€ `body` í•„ë“œì—ë§Œ ìž‘ì„±í•˜ë¼. `draft_corrected`ì˜ ë¬¸ìž¥ì„ ìˆ˜ì •/ì´ë™/ì‚­ì œí•˜ì§€ ë§ë¼.\n"
            "4) ë¬¸ë§¥ì´ ëŠê¸°ì§€ ì•Šë„ë¡ ìžì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ì™„ì„±í•˜ë¼.\n"
            "5) **ì‚¬ì‹¤ê´€ê³„ ìž„ì˜ ì¶”ê°€ ê¸ˆì§€**, í•œêµ­ì–´ë§Œ ì‚¬ìš©, ì™¸êµ­ì–´/ìž¡ë¬¸ìž ê¸ˆì§€.\n"
            "6) ìŠ¤íƒ€ì¼ ê·œì¹™ì—ì„œ ë¶„ì„ëœ ì–´ì¡° ì¤‘ ê°€ìž¥ ìœ ë ¥í•œ ì–´ì¡° í•˜ë‚˜ë§Œ ì¼ê´€ë˜ê²Œ ì ìš©í•œë‹¤.\n"
            "7) ì¶œë ¥ì€ ë‚´ë¶€ì ìœ¼ë¡œ **êµ¬ì¡°í™” ê°ì²´(ìŠ¤í‚¤ë§ˆ)**ë¡œë§Œ ìƒì„±í•œë‹¤. ê·¸ ì™¸ í…ìŠ¤íŠ¸ ìƒì„± ê¸ˆì§€.\n"
            "8) **ëª©ì°¨** {head_info}ê°€ ì •ì˜ëœ ê²½ìš° í•´ë‹¹ í—¤ë”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“ ë‹¤.ë°˜ë“œì‹œ í•´ë‹¹ í—¤ë”ë§Œ í¬í•¨í•œ ë‚´ìš©ìœ¼ë¡œ ë§Œë“ ë‹¤.\n"
             "ì‚¬ìš©ìž ì´ˆì•ˆì˜ ê¸°ì¡´ í—¤ë” êµ¬ì¡°ê°€ ë‹¤ë¥´ë”ë¼ë„ **ë°˜ë“œì‹œ {head_info} ìˆœì„œë¡œ í—¤ë”ë¥¼ ìž¬êµ¬ì„±**í•˜ë¼.\n"
            "9) **ë³¸ë¬¸ì€ ë°˜ë“œì‹œ ì´ {head_info}ì— ë§žì¶° ìž‘ì„±í•˜ë©°, Markdown í—¤ë” í‘œê¸°(#, ##, ###, ####)ë¡œ ë ˆë²¨ì„ ì •í™•ížˆ í‘œì‹œí•œë‹¤.\n"
            "10) ì „ì²´ ê¸¸ì´ëŠ” ì˜ˆì‹œ ê¸°ì¤€ ë¶„ëŸ‰ì„ ë”°ë¥¸ë‹¤(ì•½ {length_avg_chars}ìž, í—ˆìš© ë²”ìœ„ {length_min_chars}~{length_max_chars}ìž). "
            "ìµœì†Œ {length_min_chars}ìž ë¯¸ë§Œì´ ë˜ì§€ ì•Šê²Œ ì¶©ë¶„ížˆ ì„œìˆ í•˜ë¼.\n\n"
            "11) ê¸¸ì´ ê°€ì´ë“œëŠ” `draft_corrected + body`ì˜ í•©ìœ¼ë¡œ ì ìš©í•œë‹¤(ì•½ {length_avg_chars}ìž, í—ˆìš© {length_min_chars}~{length_max_chars}ìž).\n\n"
            "12) ì¶œë ¥ì€ **êµ¬ì¡°í™” ìŠ¤í‚¤ë§ˆ(NoteAgentOutput)**ë¡œë§Œ ìƒì„±í•œë‹¤. ê·¸ ì™¸ í…ìŠ¤íŠ¸ ê¸ˆì§€. `completed_text` í•„ë“œëŠ” `draft_corrected` ì™€ `body`ë¥¼ ì´ì–´ë¶™ì¸ ì™„ì„± ê¸€ì´ë‹¤.\n\n"
            "ì°¸ê³  ë¬¸ì²´ ì¡°ê°(ì˜ˆì‹œ ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼):\n{context}\n"
        ),
        (
            "human",
            "ðŸ“ ì‚¬ìš©ìž ì§€ì‹œì‚¬í•­:\n{user_input}\n\n"
            "ðŸ§¾ ì‚¬ìš©ìž ì´ˆì•ˆ(ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìžì—´ í—ˆìš©):\n{user_draft}\n\n"
            "ìš”êµ¬ì‚¬í•­:\n"
            "- ì´ì–´ì„œ **{head_info} ìˆœì„œëŒ€ë¡œ** ë³¸ë¬¸ì„ ìž‘ì„±í•˜ë˜, Markdown í—¤ë” #/##/###/####ë¥¼ ì‚¬ìš©í•˜ì—¬ ë ˆë²¨ì„ ì •í™•ížˆ ë°˜ì˜í•œë‹¤.\n"
            "- ê¸¸ì´ëŠ” {length_min_chars}~{length_max_chars}ìž ë²”ìœ„ë¡œ ë§žì¶”ê³  ê°€ëŠ¥í•˜ë©´ {length_avg_chars}ìž ê·¼ì²˜ë¡œ ìž‘ì„±í•œë‹¤.\n"
            "- ë§ˆì§€ë§‰ìœ¼ë¡œ êµì •/ì¶”ê°€/ì‚¬ì‹¤ì˜¤ë¥˜ ì—¬ë¶€ë¥¼ **êµ¬ì¡°í™”ëœ ë³€ê²½ ë¡œê·¸**ë¡œ ì œê³µí•œë‹¤.\n"
            "- ìŠ¤í‚¤ë§ˆ ì™¸ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ëŠ” ìƒì„±í•˜ì§€ ë§ë¼\n\n."
            "- **ì¤‘ìš”**\n"
            "1) ì‚¬ìš©ìž ì´ˆì•ˆì˜ ë¬¸ìž¥ ë‚´ìš©/ì˜ë¯¸ëŠ” ë³´ì¡´í•˜ë˜(ë§žì¶¤ë²•Â·ë„ì–´ì“°ê¸°Â·ë¬¸ìž¥ë¶€í˜¸ êµì •ë§Œ í—ˆìš©)\n"
            "2) í—¤ë” êµ¬ì¡°ë§Œ **{head_info}**ì— ë§žê²Œ ìž¬ë°°ì¹˜í•˜ë¼.\n"
            "3) `draft_corrected`ëŠ” ì˜¤ì§ êµì •ë§Œ í—ˆìš©, ë‚´ìš© ë³€ê²½ ê¸ˆì§€. ë³¸ë¬¸ì€ `body`ì—ë§Œ ìž‘ì„±.\n"
            "4) `change_log`ì— êµì •/ì¶”ê°€/ì‚¬ì‹¤ì˜¤ë¥˜ ì—¬ë¶€ë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ê¸°ë¡."
        ),
    ]
)


#------------------------------
# ì‚¬í›„ í™•ìž¥ í•¨ìˆ˜
#------------------------------
# ì‚¬í›„ í™•ìž¥ìš© í”„ë¡¬í”„íŠ¸
expand_prompt_marker = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ìž…ë ¥ì€ ë‘ êµ¬ê°„ìœ¼ë¡œ êµ¬ì„±ëœë‹¤.\n"
            "- {draft}: **ì ˆëŒ€ ìˆ˜ì • ê¸ˆì§€(ë¶ˆë³€)**\n"
            "- {body_text}: **ì´ êµ¬ê°„ë§Œ** ê°™ì€ í†¤ìœ¼ë¡œ ì„¸ë¶€ì„¤ëª…/ì˜ˆì‹œë¥¼ ì¶”ê°€í•´ ë³´ê°•\n\n"
            "ê·œì¹™:\n"
            "- ë¶ˆë³€ êµ¬ê°„ì€ í•œ ê¸€ìžë„ ë°”ê¾¸ì§€ ë§ˆë¼. (ì‚¬ìš©ìž ì´ˆì•ˆ í…ìŠ¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€)\n"
            "- í™•ìž¥ êµ¬ê°„ì˜ Markdown í—¤ë” êµ¬ì¡°ëŠ” ìœ ì§€í•˜ë˜, ë¬¸ë‹¨Â·ì˜ˆì‹œÂ·ê·¼ê±°ë¥¼ ë³´ê°•í•´ "
            "{target_min}~{target_max}ìž ë²”ìœ„ë¡œ í™•ìž¥í•˜ë¼.\n"
            "- í•œêµ­ì–´ë§Œ ì‚¬ìš©, ì‚¬ì‹¤ ì™œê³¡ ê¸ˆì§€.\n\n"
            "ì¶œë ¥ì€ ë°˜ë“œì‹œ (ExpandedOutput)ë¥¼ ë”°ë¼ì•¼ í•œë‹¤. "
            "ì¶”ê°€ ìž‘ì—…ì´ ì—†ëŠ” í•„ë“œ(ì˜ˆ: draft_corrected, head_info)ëŠ” ë°˜í™˜í•˜ì§€ ë§ê³ , "
            " `completed_text` í•„ë“œëŠ” `{draft}` ì™€ `body`ë¥¼ ì´ì–´ë¶™ì¸ ì™„ì„± ê¸€ì´ë‹¤."
            "ì˜¤ì§ 'body', 'completed_text', 'change_log' í•„ë“œë§Œ í¬í•¨í•˜ì—¬ ì‘ë‹µí•˜ë¼."
        ),
        ("human", "{marked_input}")
    ]
)

def expand_body_with_markers(
    result: NoteAgentOutput,
    target_min: int,
    target_max: int,
    model: str,
) -> NoteAgentOutput:
    """draftëŠ” ë¶ˆë³€, bodyë§Œ í™•ìž¥í•˜ì—¬ ìµœì¢… í…ìŠ¤íŠ¸ resultë¥¼ ë°˜í™˜."""
    editor = ChatOpenAI(model=model, temperature=0.2)
    structured_editor = editor.with_structured_output(ExpandedOutput)

    marked_input = (
        "\n--- [DRAFT START] ---\n"
        f"{result.draft_corrected}"
        "\n--- [DRAFT END] ---\n"
        "\n--- [BODY START] ---\n"
        f"{result.body}"
        "\n--- [BODY END] ---\n"
    )

    chain = expand_prompt_marker.partial(
        draft=result.draft_corrected, 
        body_text=result.body,
        target_min=target_min, 
        target_max=target_max
    ) | structured_editor

    expanded_output: ExpandedOutput = chain.invoke(
        {"marked_input": marked_input}
    )

    result.body = expanded_output.body
    result.completed_text = expanded_output.completed_text
    result.change_log = expanded_output.change_log 

    return result


#------------------------------
# ê²°ê³¼ ì¶œë ¥ í•¨ìˆ˜
#------------------------------
def finalize_with_expansion(
    result: NoteAgentOutput,
    length_info: ProfileLengthInfo,
    model: str = LLM_MODEL,
) -> NoteAgentOutput:
    """
    draft_corrected(ë¶ˆë³€) + body(í™•ìž¥ ëŒ€ìƒ)ë¥¼ ê²°í•©í•´ completed_textë¥¼ ì±„ìš´ë‹¤.
    ê¸¸ì´ê°€ ë¶€ì¡±í•  ë•Œë§Œ ë§ˆì»¤ ê¸°ë°˜ìœ¼ë¡œ bodyë§Œ í™•ìž¥.
    """
    if len(result.completed_text) >= length_info.min_chars:
        return result

    origin_length = len(result.body)
    remaining_min = max(length_info.min_chars - len(result.completed_text), 0)
    remaining_max = max(length_info.max_chars - len(result.completed_text), remaining_min + 200)

    target_min = max(remaining_min, int(0.6 * length_info.min_chars))
    target_max = max(remaining_max, target_min + 200)

    expanded_full = expand_body_with_markers(
        result=result,
        target_min=target_min,
        target_max=target_max,
        model=model,
    )

    result = expanded_full
    if hasattr(result, "change_log") and hasattr(result.change_log, "additions"):
        result.change_log.additions.append(
            f"ìµœì†Œ ë¶„ëŸ‰ ë¯¸ë‹¬ë¡œ ë³¸ë¬¸(body)ë§Œ ì‚¬í›„ í™•ìž¥(ì´ˆì•ˆ ë¶ˆë³€) ê¸°ì¡´ {origin_length}ìž -> {len(result.body)}ìž)"
        )
    return result


#------------------------------
# ì²´ì¸ í•¨ìˆ˜
#------------------------------
def build_completion_chain(
    style_rules: str,
    vs: Chroma,
    length_info: ProfileLengthInfo,
    head_info: Optional[List[HeadInfo]] = None,
    retriever_k: int = RETRIEVE_K,
    model: str = LLM_MODEL,
    temp: float = TEMPL_COMPLETE,
):
    """langchain ì²´ì¸ì„ êµ¬ì¶•í•˜ëŠ” í•¨ìˆ˜

    Args:
        style_rules (str): ìŠ¤íƒ€ì¼ ê·œì¹™
        vs (Chroma): ë²¡í„°ìŠ¤í† ì–´
        length_info (dict): ê¸¸ì´ ì •ë³´
        head_info (List[HeadInfo] | None): í—¤ë” ì •ë³´
        retriever_k (int, optional): ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜. Defaults to RETREIEVE_K.
        model (str, optional): ì‚¬ìš©í•  LLM ëª¨ë¸. Defaults to LLM_MODEL.
        temp (float, optional): LLM ì˜¨ë„. Defaults to TEMPL_COMPLETE.

    Returns:
        chain: êµ¬ì¶•ëœ langchain ì²´ì¸
    """

    retriever = vs.as_retriever(search_kwargs={"k": retriever_k})

    def _format_docs(docs):
        """
        ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìžì—´ë¡œ í¬ë§·íŒ…í•˜ëŠ” í•¨ìˆ˜
        """
        return "\n\n---\n\n".join([d.page_content for d in docs])

    def _rag_context(x: Dict[str, str]) -> str:
        q = (x.get("user_input") or "") + "\n\n" + (x.get("user_draft") or "")
        docs = retriever.invoke(q)
        return _format_docs(docs)

    llm_structured = ChatOpenAI(model=model, temperature=temp).with_structured_output(
        NoteAgentOutput
    )

    chain = (
        {
            "context": _rag_context,
            "user_input": itemgetter("user_input"),
            "user_draft": itemgetter("user_draft"),
            "style_rules": lambda _: style_rules,
            "head_info": lambda _: head_info,
            "length_avg_chars": lambda _: length_info.avg_chars,
            "length_min_chars": lambda _: length_info.min_chars,
            "length_max_chars": lambda _: length_info.max_chars,
        }
        | prompt
        | llm_structured
    )

    return chain
